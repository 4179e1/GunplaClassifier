{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18c9b3b5",
   "metadata": {},
   "source": [
    "# Gunpla Datasets\n",
    "\n",
    "We will use Dalong's Gunpla Review(http://www.dalong.net/) as train/test set, the images are shuffled and 70% of them will be training set, the rest will be test set.\n",
    "For cross validation, we'll use the data from https://acg.78dm.net/ct/3272.html. Dalong have much more images than 78dm.\n",
    "\n",
    "As a result, the traning/test will have same distribution, but cross vaidation set's distrubution is diferrent. It's not the best way to organize the data, but it's easier start with.\n",
    "\n",
    "> Note: Gunpla comes with many *grades*: HG, MG, RG, PG, etc. For simplicity, we'll start with [RG](http://dalong.net/reviews/rg.htm) since it have fewer models (37 as of this writting at 10/29/2022)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86538f6c",
   "metadata": {},
   "source": [
    "## Setup the request timeout and retries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99ecf764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "\n",
    "TIMEOUT=3\n",
    "MAX_RETRIES=3\n",
    "\n",
    "\n",
    "session = requests.Session()\n",
    "session.mount('http://', HTTPAdapter(max_retries=MAX_RETRIES))\n",
    "session.mount('https://', HTTPAdapter(max_retries=MAX_RETRIES))\n",
    "\n",
    "\n",
    "def session_get(url: str, **kwargs):\n",
    "    return session.get(url, timeout=TIMEOUT, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0603a4d0",
   "metadata": {},
   "source": [
    "# Fetching the training/test dataset from dalong.net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2eeb5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dalong's Gunpla Review(http://www.dalong.net/) contains lots of images of Gunpla, orginized by Grade, type.\n",
    "Each grade/type have a specified page, e.g:\n",
    "- RG RX-78-2:\n",
    "- MG RX-78-2:\n",
    "\"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import random\n",
    "import os\n",
    "import traceback\n",
    "import shutil\n",
    "import time\n",
    "import json\n",
    "\n",
    "from typing import Tuple, List, Optional\n",
    "\n",
    "random.seed(3.14)\n",
    "\n",
    "# The ratio of test dataset\n",
    "TEST_RATIO=0.3\n",
    "\n",
    "DATA_DIR=\"./data\"\n",
    "\n",
    "def do_download(root: str, items: List[str], verbose: Optional[bool]=False) -> None:\n",
    "    \"\"\"Download a list of urls into the specified dirctory\n",
    "    \n",
    "    Args:\n",
    "        root: the directory to save the urls\n",
    "        items: a list containing the urls, e.g. [\"http://example.com/abc.png\"]\n",
    "    \n",
    "    Returns: \n",
    "        None\n",
    "    \"\"\"\n",
    "    for url in items:\n",
    "        filename = os.path.basename(url)\n",
    "        filename = os.path.join(root, filename)\n",
    "        \n",
    "        if verbose:\n",
    "            print (f\"Downloading {url} as {filename}\")\n",
    "\n",
    "        resp = session_get(url, stream=True)\n",
    "        if resp.status_code != requests.codes.ok:\n",
    "            print (f\"Error fetching {url}, status code {resp.status_code}, continue to next item\")\n",
    "            continue\n",
    "        \n",
    "        with open(filename, 'wb') as out_file:\n",
    "            shutil.copyfileobj(resp.raw, out_file)\n",
    "        del resp\n",
    "\n",
    "        # TODO: insert a sleep here?\n",
    "        #time.sleep (0.05)\n",
    "        \n",
    "\n",
    "def download_data (\n",
    "        grade: str, \n",
    "        items: List[str], \n",
    "        verbose: Optional[bool]=False\n",
    "    ) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Download images from a gunpla review page.\n",
    "    \n",
    "    Args:\n",
    "        grade: the grade of gunpla, e.g. RG\n",
    "        code_name: code name of the type, e.g. RX-78-2 for Gundam, and MS-06S for Zaku\n",
    "        items: the urls of images associated with this grade/code_name, \n",
    "               they are suffled and split into training set and test set\n",
    "    \n",
    "    Returns: \n",
    "        a tuple contain 2 list, the 1st is the train data, and the 2nd is the test data\n",
    "    \"\"\"    \n",
    "    random.shuffle(items)        \n",
    "    divider = int(len(items) * TEST_RATIO)\n",
    "    train = items[0:-divider]\n",
    "    test = items[-divider:]\n",
    "\n",
    "    for x, data in zip([\"train\", \"test\"], [train, test]):\n",
    "        root=f\"{DATA_DIR}/gunpla/{x}/{grade}\"\n",
    "        \n",
    "        os.makedirs(root, exist_ok=True)\n",
    "   \n",
    "        do_download (root, data, verbose)\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "\n",
    "def get_rg_mg_details(url: str) -> Tuple[str, List[str]]:\n",
    "    \"\"\"Download the gunpla review page, and parse the necessary information.\n",
    "    \n",
    "    Args:\n",
    "        url: the url of a gunpla review page, e.g. http://dalong.net/reviews/rg/rg01/rg01_p.htm\n",
    "    \n",
    "    Returns: \n",
    "        A tuple containg \n",
    "        1. the code name\n",
    "        2. the urls of images associated with the gunpla\n",
    "    \"\"\"\n",
    "    resp = session_get(url)\n",
    "    if resp.status_code != requests.codes.ok:\n",
    "        print (f\"Error fetching {url}, status code {resp.status_code}\")\n",
    "        raise\n",
    "        \n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "    body = soup.body\n",
    "    t0 = body.select('table')[0]\n",
    "    c=t0.find(class_=\"CodeName\")\n",
    "    code_name = c.text.strip().replace(\"/\", \"_\") # some model contain a '/' in its codename\n",
    "    \n",
    "    f=t0.find(class_=\"FullName\")\n",
    "    full_name = f.text.strip()\n",
    "        \n",
    "    t1 = body.select('table')[3]\n",
    "    items=t1.find_all('a', href=True)\n",
    "    items=[x['href'] for x in items]\n",
    "    items=[urljoin(url, x) for x in items]\n",
    "    \n",
    "    return code_name, full_name, items\n",
    "\n",
    "def get_hg_details(url: str) -> Tuple[str, List[str]]:\n",
    "    \"\"\"Download the gunpla review page, and parse the necessary information.\n",
    "    \n",
    "    Args:\n",
    "        url: the url of a gunpla review page, e.g. http://dalong.net/reviews/rg/rg01/rg01_p.htm\n",
    "    \n",
    "    Returns: \n",
    "        A tuple containg \n",
    "        1. the code name\n",
    "        2. the urls of images associated with the gunpla\n",
    "    \"\"\"\n",
    "    resp = session_get(url)\n",
    "    if resp.status_code != requests.codes.ok:\n",
    "        print (f\"Error fetching {url}, status code {resp.status_code}\")\n",
    "        raise\n",
    "        \n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "    body = soup.body\n",
    "    t0 = body.select('table')[0]\n",
    "    c=t0.find(class_=\"CodeName\")\n",
    "    code_name = c.text.strip()\n",
    "\n",
    "    f=t0.find(class_=\"FullName\")\n",
    "    full_name = f.text.strip()\n",
    "\n",
    "        \n",
    "    t1 = body.select('table')[-3]\n",
    "    items=t1.find_all('a', href=True)\n",
    "    items=[x['href'] for x in items]\n",
    "    items=[urljoin(url, x) for x in items]\n",
    "    \n",
    "    return code_name, full_name, items\n",
    "\n",
    "# Metadata from dalong.net\n",
    "DalongData={\n",
    "    \"RG\": {\n",
    "        \"url_format\": \"http://dalong.net/reviews/rg/rg{i:0>2}/rg{i:0>2}_p.htm\",\n",
    "        # The total number of RG Gunpla\n",
    "        # It's 37 as of this writting at 10/29/2022\n",
    "        # Use a lower value during development and testing.\n",
    "        \"last_index\": 37,\n",
    "        \"details_func\": get_rg_mg_details,\n",
    "    },\n",
    "    \"MG\": {\n",
    "        \"url_format\": \"http://dalong.net/reviews/mg/m{i:0>2}/m{i:0>2}_p.htm\",\n",
    "        \"last_index\": 0,\n",
    "        \"details_func\": get_rg_mg_details,\n",
    "\n",
    "    },\n",
    "    \"HG\": {\n",
    "        \"url_format\": \"http://dalong.net/reviews/hg/h{i:0>2}/h{i:0>2}_p.htm\",\n",
    "        \"last_index\": 0,\n",
    "        \"details_func\": get_hg_details,\n",
    "\n",
    "    }\n",
    "}\n",
    "\n",
    "def download_dataset(verbose: Optional[bool]=False) -> None:\n",
    "    \"\"\"Download the entire dataset specified in DalongData\n",
    "    \n",
    "    Args:\n",
    "        None\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    metadata={}\n",
    "    \n",
    "    for grade, meta in DalongData.items():\n",
    "        # TODO: download the data\n",
    "        print(f\"=========={grade}==========\")\n",
    "        url_format=meta[\"url_format\"]\n",
    "        last_index=meta[\"last_index\"]\n",
    "        details_func=meta[\"details_func\"]\n",
    "        \n",
    "        for i in range(1, 1 + last_index):\n",
    "            url = url_format.format(i=i)\n",
    "            try:\n",
    "                code, name, items = details_func (url)\n",
    "            except:\n",
    "                print(traceback.format_exc())\n",
    "                continue\n",
    "                \n",
    "            gn = grade + \"{0:0>2}\".format(i)\n",
    "            \n",
    "            print (f\"Downloading {gn} {code} {name} url: {url}\")\n",
    "     \n",
    "            train, test = download_data (gn, items, verbose)\n",
    "            metadata[gn]={\n",
    "                \"code\": code,\n",
    "                \"name\": name,\n",
    "                \"train\": train,\n",
    "                \"test\": test,\n",
    "                \"#train\": len(train),\n",
    "                \"#test\": len(test),\n",
    "            }\n",
    "\n",
    "    \n",
    "    dump_file=f\"{DATA_DIR}/gunpla/metadata.json\"\n",
    "    with open(dump_file, 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "    \n",
    "    print (f\">>> All done, metadata written to {dump_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9e1151",
   "metadata": {},
   "source": [
    "## Getting the review page details\n",
    "\n",
    "We can run some unit test to make sure the functions are working as expected before actually downloading the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65e7c6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RX-77-2 Guncannon\n",
      "RX-78-2 Gundam\n",
      "RX-78-2 Gundam\n"
     ]
    }
   ],
   "source": [
    "code,name,items=get_hg_details('http://dalong.net/reviews/hg/h01/h01_p.htm')\n",
    "print(code, name)\n",
    "assert len(items) != 0\n",
    "\n",
    "\n",
    "code,name,items=get_rg_mg_details('http://dalong.net/reviews/mg/m01/m01_p.htm')\n",
    "print(code, name)\n",
    "assert len(items) != 0\n",
    "\n",
    "\n",
    "code,name,items=get_rg_mg_details('http://dalong.net/reviews/rg/rg01/rg01_p.htm')\n",
    "print(code, name)\n",
    "assert len(items) != 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89aabc4f",
   "metadata": {},
   "source": [
    "## Downloading few samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02de90b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://dalong.net/reviews/rg/rg01/p/rg01_02.jpg as ./data/playground/rg01_02.jpg\n",
      "Downloading http://dalong.net/reviews/rg/rg02/p/rg02_02.jpg as ./data/playground/rg02_02.jpg\n",
      "Downloading http://dalong.net/reviews/rg/rg03/p/rg03_02.jpg as ./data/playground/rg03_02.jpg\n",
      "Downloading http://dalong.net/reviews/rg/rg04/p/rg04_02.jpg as ./data/playground/rg04_02.jpg\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('./data/playground', exist_ok=True)\n",
    "\n",
    "url_format = DalongData[\"RG\"][\"url_format\"]\n",
    "\n",
    "for i in range(1, 5):\n",
    "    url = url_format.format(i=i)\n",
    "    code,name,items=get_rg_mg_details(url)\n",
    "    do_download('./data/playground', items[1:2], verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2402aaa3",
   "metadata": {},
   "source": [
    "## Download the full dataset\n",
    "\n",
    "Now let's download the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9123b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -rf ./data/gunpla\n",
    "download_dataset(verbose=False)\n",
    "!rm -rf ./data/gunpla/train/.ipynb_checkpoints\n",
    "!rm -rf ./data/gunpla/test/.ipynb_checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d403a1a0",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "Some image were failing:\n",
    "\n",
    "- Error fetching http://dalong.net/reviews/rg/rg30/p/rg30_68.jpg, status code 404, continue to next item\n",
    "- Error fetching http://dalong.net/reviews/rg/rg33/p/rg33_12.jpg, status code 404, continue to next item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed127612",
   "metadata": {},
   "source": [
    "# Fetching the validation dataset from 78dm.net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff2f0c1",
   "metadata": {},
   "source": [
    "## Data Utils for visulization and transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc3a8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def do_78dm_download(root: str, items: List[str], verbose: Optional[bool]=False) -> None:\n",
    "    for i, url in enumerate(items):\n",
    "        filename = os.path.join(root, \"{0:0>2}.jpg\".format(i))\n",
    "        \n",
    "        if verbose:\n",
    "            print (f\"Downloading {url} as {filename}\")\n",
    "\n",
    "        resp = session_get(url, stream=True)\n",
    "        if resp.status_code != requests.codes.ok:\n",
    "            print (f\"Error fetching {url}, status code {resp.status_code}, continue to next item\")\n",
    "            continue\n",
    "        \n",
    "        with open(filename, 'wb') as out_file:\n",
    "            shutil.copyfileobj(resp.raw, out_file)\n",
    "        del resp\n",
    "\n",
    "def get_78dm_details(url):\n",
    "    resp = session_get(url)\n",
    "    if resp.status_code != requests.codes.ok:\n",
    "        print (f\"Error fetching {url}, status code {resp.status_code}\")\n",
    "        raise\n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')        \n",
    "    body=soup.body\n",
    "    info=body.find(class_=\"show-info\")\n",
    "    imgs=info.find_all('img')\n",
    "    \n",
    "    items = [x['src'] for x in imgs[1:]]\n",
    "    items = [x if x.startswith('http:') else 'http:' + x for x in items]\n",
    "    \n",
    "    return items\n",
    "\n",
    "\n",
    "def _get_78dm_model_list(url: str) -> List[str]:\n",
    "    resp = session_get(url)\n",
    "    if resp.status_code != requests.codes.ok:\n",
    "        print (f\"Error fetching {url}, status code {resp.status_code}\")\n",
    "        raise\n",
    "        \n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')    \n",
    "    body=soup.body\n",
    "    content=body.find(\"div\", class_=\"is-all\")\n",
    "    links=content.find_all('a', href=True)\n",
    "    links=[x['href'] for x in links]\n",
    "    links=[urljoin(url, x) for x in links]\n",
    "    return links\n",
    "  \n",
    "\n",
    "_78dmData={\n",
    "    \"RG\": \"https://acg.78dm.net/ct/3272.html\"\n",
    "    # \"MG\": \"\",\n",
    "    #\"HG\": \"\",\n",
    "}\n",
    "\n",
    "\n",
    "def download_78dm_dataset(verbose: Optional[bool]=False) -> None:\n",
    "    for grade, url in _78dmData.items():            \n",
    "        try:\n",
    "            models = _get_78dm_model_list(url)\n",
    "        except:\n",
    "            print(traceback.format_exc())\n",
    "            continue            \n",
    "    \n",
    "        i=1\n",
    "        for model in models:\n",
    "            gn = grade + \"{0:0>2}\".format(i)\n",
    "            i+=1\n",
    "            items = get_78dm_details(model)\n",
    "            \n",
    "            root = f\"{DATA_DIR}/gunpla/validate/{gn}\"\n",
    "            os.makedirs(root, exist_ok=True)\n",
    "            \n",
    "            print (f\">>> Downloading {gn} {model} to {root}\")\n",
    "                \n",
    "            do_78dm_download(root, items, verbose=verbose)\n",
    "    print (\">>> All done <<<\")\n",
    "\n",
    "#items = get_78dm_details(\"https://acg.78dm.net/ct/6364.html\")\n",
    "#do_78dm_download(\"./data/playground\", items[:3], verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9316e99a",
   "metadata": {},
   "source": [
    "## Getting the detail page from 78dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179ab005",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = get_78dm_details(\"https://acg.78dm.net/ct/6371.html\")\n",
    "items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78225ac7",
   "metadata": {},
   "source": [
    "## Download the full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef1d71c",
   "metadata": {},
   "source": [
    "download_78dm_dataset(verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb888ad",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "Some errors\n",
    "\n",
    "```\n",
    ">>> Downloading RG25 https://acg.78dm.net/ct/112934.html to ./data/gunpla/validate/RG25\n",
    "Error fetching http://bbs-attachment-cdn.78dm.net/data/attachment/forum/201707/21/175118wr0t8ci1lm9jz9mu.jpg-w1200h1200, status code 404, continue to next item\n",
    "Error fetching http://bbs-attachment-cdn.78dm.net/data/attachment/forum/201707/21/175131qud9vvh7pwybywih.jpg-w1200h1200, status code 404, continue to next item\n",
    "Error fetching http://bbs-attachment-cdn.78dm.net/data/attachment/forum/201707/21/175143xjyezoavlzvdjvmm.jpg-w1200h1200, status code 404, continue to next item\n",
    "Error fetching http://bbs-attachment-cdn.78dm.net/data/attachment/forum/201707/21/175154e5hhzjjjikh77ti2.jpg-w1200h1200, status code 404, continue to next item\n",
    "Error fetching http://bbs-attachment-cdn.78dm.net/data/attachment/forum/201707/21/175207qxumubnmisi2suam.jpg-w1200h1200, status code 404, continue to next item\n",
    "Error fetching http://bbs-attachment-cdn.78dm.net/data/attachment/forum/201707/21/175221o5jdkf45m4xjmxjd.jpg-w1200h1200, status code 404, continue to next item\n",
    "Error fetching http://bbs-attachment-cdn.78dm.net/data/attachment/forum/201707/21/175230zo000460fjgvbb1b.jpg-w1200h1200, status code 404, continue to next item\n",
    "Error fetching http://bbs-attachment-cdn.78dm.net/data/attachment/forum/201707/21/175238orge0xfaoxgfxot4.jpg-w1200h1200, status code 404, continue to next item\n",
    "Error fetching http://bbs-attachment-cdn.78dm.net/data/attachment/forum/201707/21/175247q5rlhw5hlrroihlt.jpg-w1200h1200, status code 404, continue to next item\n",
    "Error fetching http://bbs-attachment-cdn.78dm.net/data/attachment/forum/201707/21/175343smfaczpp1z4cifnm.jpg-w1200h1200, status code 404, continue to next item\n",
    "Error fetching http://bbs-attachment-cdn.78dm.net/data/attachment/forum/201707/21/175407zq0zdsehshsaqden.jpg-w1200h1200, status code 404, continue to next item\n",
    "Error fetching http://bbs-attachment-cdn.78dm.net/data/attachment/forum/201707/21/175416nioggpdqop44tpyg.jpg-w1200h1200, status code 404, continue to next item\n",
    "Error fetching http://bbs-attachment-cdn.78dm.net/data/attachment/forum/201707/21/175427i4oqmvxfvd3y3ygx.jpg-w1200h1200, status code 404, continue to next item\n",
    "Error fetching http://bbs-attachment-cdn.78dm.net/data/attachment/forum/201707/21/175453v1srm1rs43kv6h36.jpg-w1200h1200, status code 404, continue to next item\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8206114c",
   "metadata": {},
   "source": [
    "# Data Visulization\n",
    "\n",
    "The code snippet will be written into the file data_utils, so that it can be re-used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ef3fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile data_utils.py\n",
    "\n",
    "from typing import List, Union, Optional\n",
    "\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import torch\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "\n",
    "def get_random_image(image_dir: str, pattern: str=\"*.jpg\") -> pathlib.Path:\n",
    "    \"\"\"\n",
    "    Get a random image path from given directory.\n",
    "    \n",
    "    Args:\n",
    "        image_dir: the direcory to look at\n",
    "        pattern: the pattern of the filename\n",
    "    \n",
    "    Return:\n",
    "        The path of the random selected image\n",
    "        \n",
    "    Note: \n",
    "        The lookup is recursive, for example, withe image_dir=\"./\" and pattern =\"*.jpg\",\n",
    "        it could match \"./abc.jpg\" or \"./subdir/def.jpg\" \n",
    "        \n",
    "    \"\"\"\n",
    "    path = pathlib.Path(image_dir)\n",
    "\n",
    "    # 1. Get all image paths\n",
    "    image_path_list = list(path.rglob(pattern))\n",
    "    \n",
    "    # 2. Get random image path\n",
    "    img_path = random.choice (image_path_list)\n",
    "    return img_path\n",
    "\n",
    "def get_random_images(image_dir: str, k: int=3, pattern: str=\"*.jpg\") -> List[pathlib.Path]:\n",
    "    \"\"\"\n",
    "    Get k random image path from given directory.\n",
    "    \n",
    "    Args:\n",
    "        image_dir: the direcory to look at\n",
    "        k: the number of image to select\n",
    "        pattern: the pattern of the filename\n",
    "    \n",
    "    Return:\n",
    "        The path of the random selected image\n",
    "        \n",
    "    Note: \n",
    "        The lookup is recursive, for example, withe image_dir=\"./\" and pattern =\"*.jpg\",\n",
    "        it could match \"./abc.jpg\" or \"./subdir/def.jpg\" \n",
    "        \n",
    "    \"\"\"\n",
    "    path = pathlib.Path(image_dir)\n",
    "\n",
    "    # 1. Get all image paths\n",
    "    image_path_list = list(path.rglob(pattern))\n",
    "    \n",
    "    # 2. Get random image path\n",
    "    pathes = random.choices (image_path_list, k=k)\n",
    "    \n",
    "    return pathes\n",
    "\n",
    "def get_image_class(img_path: Union[pathlib.Path, str]) -> str:\n",
    "    \"\"\"\n",
    "    Given a path to image inside a standard Pytorch ImageFolder \n",
    "    (https://pytorch.org/vision/stable/generated/torchvision.datasets.ImageFolder.html),\n",
    "    return the class of that image.\n",
    "    \n",
    "    Args:\n",
    "        img_path: the path to the image, for example \"root/dog/xxx.png\"\n",
    "        \n",
    "    Returns:\n",
    "        the class of that image, it returns \"dog\" in the example above\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make sure it's an instance of pathlib.Path\n",
    "    p = pathlib.Path (img_path)\n",
    "    \n",
    "    return p.parent.stem\n",
    "    \n",
    "    \n",
    "def display_image(img_path: Union[pathlib.Path, str]) -> None:\n",
    "    \"\"\"\n",
    "    Dispaly the specified image in IPython console, \n",
    "    also print it's path, height, width\n",
    "    \n",
    "    Args:\n",
    "        image_path: the path of the image\n",
    "    \n",
    "    Return:\n",
    "        None\n",
    "           \n",
    "    Note:\n",
    "        This works only in IPython console.\n",
    "    \"\"\"\n",
    "\n",
    "    img = Image.open(img_path)\n",
    "\n",
    "    # 5. Print metadata\n",
    "    print(f\"Image path: {img_path}\")\n",
    "    print(f\"Image height: {img.height}\") \n",
    "    print(f\"Image width: {img.width}\")\n",
    "    \n",
    "    \n",
    "    display(img)\n",
    "        \n",
    "def _get_image_data (\n",
    "        image: Union[pathlib.Path, str, Image.Image, torch.Tensor]\n",
    "    ) -> np.ndarray:\n",
    "\n",
    "    if isinstance (image, str) or isinstance (image, pathlib.Path):\n",
    "        image = Image.open(image)\n",
    "\n",
    "    if isinstance(image, Image.Image):\n",
    "        image = np.asarray(image)\n",
    "        return image\n",
    "    elif isinstance(image, torch.Tensor):\n",
    "        # converting Tensor to numpy is not necessary, but make sure it always return a numpy.ndarray\n",
    "        image = image.permute(1, 2, 0).numpy()\n",
    "        return image\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported type {type(image)}\")\n",
    "\n",
    "def plot_image(image: Union[pathlib.Path, str, Image.Image, torch.Tensor], title: Optional[str]=None) -> None:\n",
    "    \"\"\"\n",
    "    Plot image with matplotlib\n",
    "    \n",
    "    Args:\n",
    "        image: either the path of image, \n",
    "               or the image opened as PIL.Image.Image (via PIL.Image.Open),\n",
    "               or the image transformed to pytorch tensor\n",
    "    \n",
    "    Return:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    data = _get_image_data(image)\n",
    "\n",
    "    plt.imshow(data)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def plot_images(\n",
    "        images: List[Union[pathlib.Path, str, Image.Image, torch.Tensor]], \n",
    "        titles: Optional[List[str]]=None\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Plot image with matplotlib, all images are show in a figure horizontally\n",
    "    \n",
    "    Args:\n",
    "        images: a list containing the image data\n",
    "        titles: a list containint the title of each image\n",
    "    \n",
    "    Return:\n",
    "        None\n",
    "        \n",
    "    Note:\n",
    "        If not None, the length of titles must match the length of img_paths\n",
    "    \"\"\"\n",
    "    if titles:\n",
    "        assert len(images) == len(titles)\n",
    "    else:\n",
    "        titles = [None] * len(images)\n",
    "    \n",
    "    ncols = len(images)\n",
    "    \n",
    "    fig, axs = plt.subplots(ncols=ncols, figsize=(ncols*6,6))\n",
    "    \n",
    "    for ax, i, t in zip (axs, images, titles):\n",
    "        image = _get_image_data (i)\n",
    "        ax.imshow(image)\n",
    "        if t:\n",
    "            ax.title.set_text(t)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "        \n",
    "def show_random_image(image_dir: str, pattern: str=\"*.jpg\") -> None:\n",
    "    \"\"\"\n",
    "    Show an image randomly selected from given directory.\n",
    "    \n",
    "    Args:\n",
    "        image_dir: the direcory to look at\n",
    "        pattern: the pattern of the filename\n",
    "    \n",
    "    Return:\n",
    "        None\n",
    "        \n",
    "    Note: \n",
    "        The lookup is recursive, for example, withe image_dir=\"./\" and pattern =\"*.jpg\",\n",
    "        it could match \"./abc.jpg\" or \"./subdir/def.jpg\" \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    img_path = get_random_image(image_dir, pattern)\n",
    "    label = get_image_class (img_path)\n",
    "    plot_image(img_path, label)\n",
    "    \n",
    "def show_random_images(image_dir: str, k: int=3, pattern=\"*.jpg\"):\n",
    "    \"\"\"\n",
    "    Show k images randomly selected from given directory.\n",
    "    \n",
    "    Args:\n",
    "        image_dir: the direcory to look at\n",
    "        k: the number of images to show\n",
    "        pattern: the pattern of the filename\n",
    "    \n",
    "    Return:\n",
    "        None\n",
    "        \n",
    "    Note: \n",
    "        The lookup is recursive, for example, withe image_dir=\"./\" and pattern =\"*.jpg\",\n",
    "        it could match \"./abc.jpg\" or \"./subdir/def.jpg\" \n",
    "        \n",
    "    \"\"\"\n",
    "    img_paths = get_random_images(image_dir, k, pattern)\n",
    "    labels = [get_image_class(x) for x in img_paths]\n",
    "    plot_images(img_paths, labels)\n",
    "    \n",
    "def plot_patched_image (image: Union[pathlib.Path, str, Image.Image, torch.Tensor], patch_size: int=16):\n",
    "    \n",
    "    img = _get_image_data(image)\n",
    "    assert img.shape[0] == img.shape[1], \"Image must be square\"\n",
    "    img_size = img.shape[1]\n",
    "    \n",
    "    assert img_size % patch_size == 0, \"Image size must be divisible by patch size\" \n",
    "    num_patches = img_size/patch_size\n",
    "    print(f\"Image size: {img.shape[0]} x {img.shape[1]}\\\n",
    "        \\nNumber of patches per row: {num_patches}\\\n",
    "        \\nNumber of patches per column: {num_patches}\\\n",
    "        \\nTotal patches: {num_patches*num_patches}\\\n",
    "        \\nPatch size: {patch_size} pixels x {patch_size} pixels\")\n",
    "    \n",
    "    fig, axs = plt.subplots(nrows=img_size//patch_size, # need int\n",
    "            ncols=img_size//patch_size,\n",
    "            figsize=(num_patches, num_patches),\n",
    "            sharex=True,\n",
    "            sharey=True,\n",
    "    )\n",
    "    \n",
    "    for i, h in enumerate(range(0, img_size, patch_size)):\n",
    "        for j, w in enumerate(range(0, img_size, patch_size)):\n",
    "            ax = axs[i, j]\n",
    "            ax.imshow(img[h:h+patch_size, w:w+patch_size, :])\n",
    "            \n",
    "            ax.set_ylabel(i+1, \n",
    "                        rotation=\"horizontal\", \n",
    "                        horizontalalignment=\"right\",\n",
    "                        verticalalignment=\"center\")\n",
    "            ax.set_xlabel(j+1)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.label_outer()\n",
    "            \n",
    "    fig.suptitle(\"Image Patchified\", fontsize=16)\n",
    "    plt.show()\n",
    "    \n",
    "def show_image_statistics(image_dir, pattern=\"*.jpg\"):\n",
    "    \"\"\"\n",
    "    Show statistics of all matched images under image_dir\n",
    "    \n",
    "    Args:\n",
    "        image_dir: the direcory to look at\n",
    "        pattern: the pattern of the filename\n",
    "    \n",
    "    Return:\n",
    "        None\n",
    "        \n",
    "    Note: \n",
    "        The lookup is recursive, for example, withe image_dir=\"./\" and pattern =\"*.jpg\",\n",
    "        it could match \"./abc.jpg\" or \"./subdir/def.jpg\" \n",
    "        \n",
    "    \"\"\"\n",
    "    path = Path(image_dir)\n",
    "    \n",
    "    # 1. Get all image paths\n",
    "    image_path_list = list(path.rglob(pattern))\n",
    "    \n",
    "    c = Counter()\n",
    "    wh=[]\n",
    "    for x in image_path_list:\n",
    "        img = Image.open(x)\n",
    "        c[x.parent.stem] += 1\n",
    "\n",
    "        wh.append([img.width, img.height])\n",
    "        del img\n",
    "    \n",
    "    t=torch.tensor(wh, dtype=torch.float)\n",
    "    mean = t.mean(dim=0)\n",
    "    stddev = t.std(dim=0)\n",
    "    median,_ = t.median(dim=0)\n",
    "    minimal, _ = t.min(dim=0) # return value,index\n",
    "    maximal, _ = t.max(dim=0)\n",
    "    \n",
    "    print (f\"# of images: {len(image_path_list)}\")\n",
    "    \n",
    "    for k in c:\n",
    "        print (f\"\\t{k:20}: {c[k]}\")\n",
    "\n",
    "    print(\"Avg \", mean)\n",
    "    print(\"Std \", stddev)\n",
    "    print(\"Med \", median)\n",
    "\n",
    "    print('Min ', minimal)\n",
    "    print('Max ', maximal)\n",
    "    \n",
    "\n",
    "    plt.xlabel('width')\n",
    "    plt.ylabel('height')\n",
    "    # WTF, see https://stackoverflow.com/questions/41248767/splitting-a-2-dimensional-array-or-a-list-into-two-1-dimensional-lists-in-python\n",
    "    w,h = zip(*wh)\n",
    "    plt.scatter(w,h)\n",
    "    plt.plot(*mean.tolist(), marker=\"o\", markersize=10, markerfacecolor=\"green\", label=\"mean\")\n",
    "    plt.plot(*median.tolist(), marker=\"o\", markersize=10, markerfacecolor=\"yellow\", label=\"median\")\n",
    "\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea656d5",
   "metadata": {},
   "source": [
    "The library can plot images in various format\n",
    "- an filename\n",
    "- PIL Image\n",
    "- or Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6293540f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import data_utils\n",
    "from torchvision import transforms\n",
    "\n",
    "img = data_utils.get_random_image(\"./data/gunpla\", \"*.jpg\")\n",
    "print (\"image path\")\n",
    "data_utils.plot_image(img)\n",
    "\n",
    "pil_img = PIL.Image.open(img)\n",
    "print (\"PIL Image\")\n",
    "data_utils.plot_image(pil_img)\n",
    "\n",
    "to_tensor = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "tensor_img = to_tensor(pil_img)\n",
    "print (\"Tensor Image\")\n",
    "data_utils.plot_image(tensor_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e748b5d6",
   "metadata": {},
   "source": [
    "## Select and plot random images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe7bcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_utils\n",
    "img = data_utils.get_random_image(\"./data/gunpla\", \"*.jpg\")\n",
    "print(type(img), img)\n",
    "imgs = data_utils.get_random_images(\"./data/gunpla\", 3, \"*.jpg\")\n",
    "print (imgs)\n",
    "\n",
    "c = data_utils.get_image_class(img)\n",
    "print (type(c), c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e01a039",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [data_utils.get_image_class(x) for x in imgs]\n",
    "data_utils.plot_images(imgs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e27d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_utils.show_random_image(\"./data/gunpla\")\n",
    "data_utils.show_random_image(\"./data/gunpla/test/RG01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1eb9d6",
   "metadata": {},
   "source": [
    "## Data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cc3a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_utils.show_image_statistics(\"./data/gunpla/\", \"*.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec17dcc",
   "metadata": {},
   "source": [
    "From the result above, we can find that the image vary in size, which make it difficult for training, as most model take a square input (image's width equals to height).\n",
    "\n",
    "Instead of stretching images, in genearl is better to pad them, here comes the `SquarePad` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cd619e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rect_img = PIL.Image.open(\"./data/playground/rg01_02.jpg\")\n",
    "\n",
    "squared_padded = data_utils.SquarePad(fill=255)(rect_img)\n",
    "\n",
    "data_utils.plot_images([rect_img, squared_padded], [\"rectengle image\", \"Square padded image\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1786c193",
   "metadata": {},
   "source": [
    "# Creating the Pytorch datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16971e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dataset.py\n",
    "import os\n",
    "import torch\n",
    "import PIL\n",
    "import json\n",
    "\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "DEFAULT_TRAIN_DIR=\"./data/gunpla/train\"\n",
    "DEFAULT_TEST_DIR=\"./data/gunpla/test\"\n",
    "DEFAULT_VALIDATE_DIR=\"./data/gunpla/validate\"\n",
    "DEFAULT_METADATA=\"./data/gunpla/metadata.json\"\n",
    "\n",
    "def create_dataset(\n",
    "        train_dir: str = DEFAULT_TRAIN_DIR,\n",
    "        test_dir: str = DEFAULT_TEST_DIR,\n",
    "        validate_dir: str = DEFAULT_VALIDATE_DIR,\n",
    "\n",
    "        train_transform: Optional[transforms.Compose] = None,\n",
    "        test_transform: Optional[transforms.Compose] = None,\n",
    "        validate_transform: Optional[transforms.Compose] = None,\n",
    "    \n",
    "        metadata: str = DEFAULT_METADATA,\n",
    "\n",
    "    ) -> Tuple[datasets.ImageFolder, datasets.ImageFolder, datasets.ImageFolder, List[str]]:\n",
    "    \"\"\"Creates training and testing dataset.\n",
    "    \n",
    "    Takes in a training directory and testing directory path and turns\n",
    "    them into PyTorch Datasets\n",
    "    \n",
    "    Args:\n",
    "      train_dir: Path to training directory.\n",
    "      test_dir: Path to testing directory.\n",
    "      train_transform: optional torchvision transforms to perform on training data.\n",
    "      test_transform: optional torchvision transforms to perform on testing data.\n",
    "\n",
    "    \n",
    "    Returns:\n",
    "      A tuple of (train_dataset, test_dataset, class_names).\n",
    "      Where class_names is a list of the target classes.\n",
    "    \"\"\"\n",
    "    train_data = datasets.ImageFolder(train_dir, transform=train_transform)\n",
    "    test_data = datasets.ImageFolder(test_dir, transform=test_transform)\n",
    "    validate_data = datasets.ImageFolder(validate_dir, transform=validate_transform)\n",
    "\n",
    "    \n",
    "    class_names = train_data.classes\n",
    "    \n",
    "    with open(metadata) as f:\n",
    "        meta = json.load(f)\n",
    "        \n",
    "    items=[]\n",
    "    for x in class_names:\n",
    "        m=meta[x]\n",
    "        item=x\n",
    "        code,name = m[\"code\"], m[\"name\"]\n",
    "        if code:\n",
    "            item += f\" {code}\"\n",
    "        if name:\n",
    "            item += f\" {name}\"\n",
    "            \n",
    "        items.append(item)\n",
    "        \n",
    "    return train_data, test_data, validate_data, items\n",
    "\n",
    "def create_dataloaders(\n",
    "        train_dir: str = DEFAULT_TRAIN_DIR,\n",
    "        test_dir: str = DEFAULT_TEST_DIR,\n",
    "        validate_dir: str = DEFAULT_VALIDATE_DIR,\n",
    "\n",
    "        train_transform: Optional[transforms.Compose] = None,\n",
    "        test_transform: Optional[transforms.Compose] = None,\n",
    "        validate_transform: Optional[transforms.Compose] = None,\n",
    "        batch_size: Optional[int]=32,\n",
    "        num_workers: Optional[int]=NUM_WORKERS,\n",
    "        metadata: str=DEFAULT_METADATA,\n",
    ") -> Tuple[DataLoader, DataLoader, DataLoader, List[str]]:\n",
    "\n",
    "    \"\"\"Creates training and testing dataloader.\n",
    "    \n",
    "    Takes in a training directory and testing directory path and turns\n",
    "    them into PyTorch DataloLoader\n",
    "    \n",
    "    Args:\n",
    "      train_dir: Path to training directory.\n",
    "      test_dir: Path to testing directory.\n",
    "      train_transform: optional torchvision transforms to perform on training data.\n",
    "      test_transform: optional torchvision transforms to perform on testing data.\n",
    "\n",
    "    \n",
    "    Returns:\n",
    "      A tuple of (train_dataloader, test_dataloader, class_names).\n",
    "      Where class_names is a list of the target classes.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_data, test_data, validate_data, class_names = create_dataset(train_dir, test_dir, validate_dir, train_transform, test_transform, validate_transform)\n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    test_dataloader = DataLoader(\n",
    "        test_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    validate_dataloader = DataLoader(\n",
    "        validate_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    return train_dataloader, test_dataloader, validate_dataloader, class_names\n",
    "\n",
    "class SquarePad(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Taken from https://discuss.pytorch.org/t/how-to-resize-and-pad-in-a-torchvision-transforms-compose/71850/10\n",
    "    With modification\n",
    "    \"\"\"\n",
    "    def __init__(self, fill: int=0):\n",
    "        super().__init__()\n",
    "        self.fill = fill\n",
    "        \n",
    "    def __call__(self, image: PIL.Image.Image):\n",
    "        max_wh = max(image.size)\n",
    "        p_left, p_top = [(max_wh - s) // 2 for s in image.size]\n",
    "        p_right, p_bottom = [max_wh - (s+pad) for s, pad in zip(image.size, [p_left, p_top])]\n",
    "        padding = (p_left, p_top, p_right, p_bottom)\n",
    "        return F.pad(image, padding, fill=self.fill, padding_mode='constant')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227ab6a5",
   "metadata": {},
   "source": [
    "## Creating DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9ae6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./data/gunpla/train/.ipynb_checkpoints\n",
    "!rm -rf ./data/gunpla/test/.ipynb_checkpoints\n",
    "!rm -rf ./data/gunpla/validate/.ipynb_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6088df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset\n",
    "train_dataloader, test_dataloader, validate_dataloader, names = dataset.create_dataloaders()\n",
    "print (names)\n",
    "print(f\"Total classes: {len(names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f2e62a",
   "metadata": {},
   "source": [
    "## Explore the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8db317",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Training set: \", train_dataloader.dataset)\n",
    "print (\"Test set: \", test_dataloader.dataset)\n",
    "print (\"Validate set: \", validate_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d22645",
   "metadata": {},
   "source": [
    "## Data Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412f9449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_utils\n",
    "\n",
    "selected = random.choices(train_dataloader.dataset, k=3)\n",
    "\n",
    "test_transform1 = transforms.Compose([\n",
    "    dataset.SquarePad(fill=255),\n",
    "    transforms.Resize((224,224)),\n",
    "    #transforms.ToTensor(),\n",
    "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "trans_count=1\n",
    "for x, class_id in selected:\n",
    "    \n",
    "    images = [x, test_transform1 (x)]\n",
    "    titles = [f\"Origin ({names[class_id]})\", \"Transformed\"]\n",
    "    \n",
    "    data_utils.plot_images(images, titles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca95d9a",
   "metadata": {},
   "source": [
    "Now let's convert them to tensor and add Normalization as required by many models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f346447",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform2 = transforms.Compose([\n",
    "    dataset.SquarePad(fill=255),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "for x, class_id in selected:\n",
    "    \n",
    "    images = [x, test_transform2 (x)]\n",
    "    titles = [f\"Origin ({names[class_id]})\", \"Transformed\"]\n",
    "    \n",
    "    data_utils.plot_images(images, titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c988d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform3 = transforms.Compose([\n",
    "    dataset.SquarePad(fill=255),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.TrivialAugmentWide(num_magnitude_bins=31),\n",
    "])\n",
    "\n",
    "\n",
    "for x, class_id in selected:\n",
    "    \n",
    "    images = [x, test_transform3 (x)]\n",
    "    titles = [f\"Origin ({names[class_id]})\", \"Transformed\"]\n",
    "    \n",
    "    data_utils.plot_images(images, titles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
